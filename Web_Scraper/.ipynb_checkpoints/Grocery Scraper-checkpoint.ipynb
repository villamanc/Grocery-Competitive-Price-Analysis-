{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "3c9d49d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "import time\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "\n",
    "def login_instacart(home_page): #locate, and enter the login information\n",
    "    driver.get(home_page)\n",
    "    time.sleep(15)\n",
    "    email = driver.find_element('xpath',\"//input[contains(@type,'email')]\")\n",
    "    email.send_keys(#Enter Acct User Name as String)\n",
    "    password = driver.find_element('xpath', \"//input[contains(@type,'password')]\")\n",
    "    password.send_keys(#Enter Acct Password as String)\n",
    "    time.sleep(15)\n",
    "    login = driver.find_element('xpath',\"//button[contains(@class,'e-ztomkz')]\")\n",
    "    login.click()\n",
    "    time.sleep(15)\n",
    "\n",
    "def scroll_bottom_of_page():#load the entire web page by scrolling to the bottom\n",
    "    pause_time_to_load = 30    \n",
    "    last_height = driver.execute_script(\"return document.body.scrollHeight\")\n",
    "    while True:\n",
    "        driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")#scroll to bottom\n",
    "        time.sleep(pause_time_to_load)\n",
    "        try:  #remove hidden element that prevents from clicking on load more button and scrolling to bottom\n",
    "            driver.find_element('xpath',\"//div[contains(@class,'e-1wf5ba')]\").remove()\n",
    "        except:\n",
    "            pass\n",
    "        try: #click load more button if it is there\n",
    "            Load_More_Button = driver.find_element('xpath',\"//span[contains(@class,'e-15utg5h')]\")\n",
    "            Load_More_Button.click()\n",
    "        except:\n",
    "            pass \n",
    "        new_height = driver.execute_script(\"return document.body.scrollHeight\")\n",
    "        if new_height == last_height: #if you can't scroll anymore break the loop else keep scrolling\n",
    "            time.sleep(20)\n",
    "            break\n",
    "        last_height = new_height\n",
    "        \n",
    "def find_prices(soup): #load the entire web page in bs4, and locate the prices\n",
    "    prices = []\n",
    "    for price in soup.find_all('div', class_ = 'e-13udsys'):\n",
    "        try:\n",
    "            prices.append(price.find('div', class_ = 'e-k008qs')[\"aria-label\"])\n",
    "        except:\n",
    "            prices.append(None) \n",
    "    return prices\n",
    "\n",
    "def find_titles(soup): #load the entire web page in bs4, and locate the titles\n",
    "    titles = []\n",
    "    for title in soup.find_all('div', class_ = 'e-13udsys'):\n",
    "        try:\n",
    "            titles.append(title.find('span', class_ = 'e-th3ch3').contents[0])\n",
    "        except:\n",
    "            titles.append(None) \n",
    "    return titles\n",
    "\n",
    "def find_quantitys(soup): #load the entire web page in bs4, and locate the quantitys\n",
    "    qauntitys = []\n",
    "    for qauntity in soup.find_all('div', class_ = 'e-13udsys'):\n",
    "        try:\n",
    "            qauntitys.append(qauntity.find('div', class_ = 'e-k9ly30')['title'])\n",
    "        except:\n",
    "            qauntitys.append(None)\n",
    "    return qauntitys\n",
    "\n",
    "def store_data(item,store,prices,titles,qauntitys): #store data into an excel sheet\n",
    "    mined_data = pd.DataFrame(\n",
    "        {'Prices': prices,\n",
    "            \"Title\": titles,\n",
    "            'Quantities': qauntitys \n",
    "        }) #populate data frame with data\n",
    "    store_directory = '/'+store.rsplit('/', 2)[1]\n",
    "    store_col = []\n",
    "    item_col = []\n",
    "    for price in prices: #add two columns of extra detail\n",
    "        store_col.append(store_directory[1:]) #fill an entire column with the store being mined for instance make a column that just says \"walmart\"\n",
    "        item_col.append(item) #fill an entire column with the item search being mined for instance make a column that just says \"seafood\"\n",
    "    mined_data[\"Store\"] = store_col\n",
    "    mined_data[\"Item\"] = item_col \n",
    "    store_directory = '/'+store.rsplit('/', 2)[1]\n",
    "    if os.path.exists(os.getcwd()+store_directory) == False:\n",
    "        os.makedirs(os.getcwd()+store_directory)\n",
    "    final_destination = os.getcwd()+store_directory\n",
    "    mined_data.to_excel(final_destination+'/'+item+'.xlsx') #store the data frame in excel\n",
    "    \n",
    "home_page = 'https://www.instacart.com/store/?categoryFilter=homeTabForYou'\n",
    "\n",
    "stores = ['https://www.instacart.com/store/big-y/storefront', \n",
    "          'https://www.instacart.com/store/stop-shop/storefront', \n",
    "          'https://www.instacart.com/store/shoprite/storefront',\n",
    "          'https://www.instacart.com/store/target/storefront']\n",
    "\n",
    "search_keys = ['Fruits','Vegetables','Canned Goods','Dairy',\n",
    "         'Meat','Fish & Seafood','Deli','Condiments & Spices','Snacks',\n",
    "         'Bread & Bakery','Beverages','Pasta, Rice & Cereal','Baking','Frozen Foods']\n",
    "\n",
    "#load driver, and log into instacart\n",
    "options = webdriver.FirefoxOptions()\n",
    "options.add_argument('--ignore-certificate-errors')\n",
    "options.add_argument('--private')\n",
    "options.add_argument('--headless')\n",
    "driver = webdriver.Firefox(options)\n",
    "driver.maximize_window()\n",
    "login_instacart(home_page)\n",
    "#repeat the scraping process for all the stores in the list of stores\n",
    "for store in stores: \n",
    "    driver.get(store)\n",
    "    time.sleep(20)\n",
    "    #scrape by searching for an item, loading the entire page, and retrieving all the data on that page\n",
    "    for Item in search_keys:\n",
    "        time.sleep(20)\n",
    "        search_bar = driver.find_element('xpath',\"//input[contains(@class,'e-13nun8m')]\")\n",
    "        time.sleep(20)\n",
    "        search_bar.send_keys(Item + Keys.ENTER)\n",
    "        time.sleep(20)\n",
    "        scroll_bottom_of_page()\n",
    "        page_source = driver.page_source\n",
    "        soup = BeautifulSoup(page_source,'lxml')\n",
    "        Prices = find_prices(soup)\n",
    "        time.sleep(5)\n",
    "        Qauntitys = find_quantitys(soup)\n",
    "        time.sleep(5)\n",
    "        Titles = find_titles(soup)\n",
    "        store_data(Item,store,Prices,Titles,Qauntitys)\n",
    "        time.sleep(5)\n",
    "        driver.back()\n",
    "    \n",
    "driver.quit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c5136f00",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "#locate all the store folders, then combine all the indidivual excel sheets into one master sheet for each store\n",
    "directory_path = os.getcwd()\n",
    "if os.path.exists(directory_path) and os.path.isdir(directory_path):\n",
    "    folder_paths = []\n",
    "    for root, dirs, files in os.walk(directory_path):\n",
    "        for folder in dirs:\n",
    "            folder_paths.append(os.path.join(root, folder))\n",
    "    #for all the files in the current directory, only open the store directorys\n",
    "    for folder in folder_paths:\n",
    "        if folder.rsplit('\\\\', 1)[1] == '.ipynb_checkpoints':\n",
    "            pass\n",
    "        else:\n",
    "            excel_directory = folder\n",
    "            dataframes = []\n",
    "            #locate all the files in the store directory, and only open excel files into a pandas data frame\n",
    "            for filename in os.listdir(excel_directory):\n",
    "                if filename.endswith('.xlsx') or filename.endswith('.xls'):  \n",
    "                    file_path = os.path.join(excel_directory, filename)\n",
    "                    df = pd.read_excel(file_path) \n",
    "                    dataframes.append(df)\n",
    "            #combine all the data frames into one store master frame\n",
    "            combined_dataframe = pd.concat(dataframes, ignore_index=True)           \n",
    "            combined_dataframe = combined_dataframe.drop(['Unnamed: 0'], axis = 1)\n",
    "            combined_dataframe = combined_dataframe.dropna(subset=['Prices'])\n",
    "            combined_dataframe.to_excel(folder+'\\\\'+folder.rsplit('\\\\')[-1]+'_All_Data.xlsx')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
